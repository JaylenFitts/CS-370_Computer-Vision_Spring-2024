{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrfeHl_-m4V-"
   },
   "source": [
    "## Install starter code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyQblYp0nEZq"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/amgyr/CS711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28bBK1v7nJOu"
   },
   "source": [
    "## Setup code\n",
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUCKw4Tl1ddj"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import coutils\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhqpd2IN2O-K"
   },
   "source": [
    "Starting in this assignment, we will use the GPU to accelerate our computation. Run this cell to make sure you are using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGDxdBIMRX6b"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdMnUocj7Srw"
   },
   "source": [
    "Now, we will load CIFAR10 dataset, with normalization.\n",
    "\n",
    "In this notebook we will use the **bias trick**: By adding an extra constant feature of ones to each image, we avoid the need to keep track of a bias vector; the bias will be encoded as the part of the weight matrix that interacts with the constant ones in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2mFlFmQ1ddm"
   },
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(validation_ratio = 0.02):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  X_train, y_train, X_test, y_test = coutils.data.cifar10()\n",
    "\n",
    "  # Move all the data to the GPU\n",
    "  X_train = X_train.cuda()\n",
    "  y_train = y_train.cuda()\n",
    "  X_test = X_test.cuda()\n",
    "  y_test = y_test.cuda()\n",
    "\n",
    "  # 0. Visualize some examples from the dataset.\n",
    "  class_names = [\n",
    "      'plane', 'car', 'bird', 'cat', 'deer',\n",
    "      'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "  ]\n",
    "  img = coutils.utils.visualize_dataset(X_train, y_train, 12, class_names)\n",
    "  plt.imshow(img)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "  # 1. Normalize the data: subtract the mean RGB (zero mean)\n",
    "  mean_image = X_train.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "  X_train -= mean_image\n",
    "  X_test -= mean_image\n",
    "\n",
    "  # 2. Reshape the image data into rows\n",
    "  X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "  X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "  # 3. Add bias dimension and transform into columns\n",
    "  ones_train = torch.ones(X_train.shape[0], 1, device=X_train.device)\n",
    "  X_train = torch.cat([X_train, ones_train], dim=1)\n",
    "  ones_test = torch.ones(X_test.shape[0], 1, device=X_test.device)\n",
    "  X_test = torch.cat([X_test, ones_test], dim=1)\n",
    "\n",
    "  # 4. Carve out part of the training set to use for validation.\n",
    "  # For random permutation, you can use torch.randperm or torch.randint\n",
    "  # But, for this homework, we use slicing instead.\n",
    "  num_training = int( X_train.shape[0] * (1.0 - validation_ratio) )\n",
    "  num_validation = X_train.shape[0] - num_training\n",
    "\n",
    "  # Return the dataset as a dictionary\n",
    "  data_dict = {}\n",
    "  data_dict['X_val'] = X_train[num_training:num_training + num_validation]\n",
    "  data_dict['y_val'] = y_train[num_training:num_training + num_validation]\n",
    "  data_dict['X_train'] = X_train[0:num_training]\n",
    "  data_dict['y_train'] = y_train[0:num_training]\n",
    "\n",
    "  data_dict['X_test'] = X_test\n",
    "  data_dict['y_test'] = y_test\n",
    "  return data_dict\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "data_dict = get_CIFAR10_data()\n",
    "print('Train data shape: ', data_dict['X_train'].shape)\n",
    "print('Train labels shape: ', data_dict['y_train'].shape)\n",
    "print('Validation data shape: ', data_dict['X_val'].shape)\n",
    "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
    "print('Test data shape: ', data_dict['X_test'].shape)\n",
    "print('Test labels shape: ', data_dict['y_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biGULpeRxEhe"
   },
   "outputs": [],
   "source": [
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-7):\n",
    "  \"\"\"\n",
    "  Utility function to perform numeric gradient checking. We use the centered\n",
    "  difference formula to compute a numeric derivative:\n",
    "  \n",
    "  f'(x) =~ (f(x + h) - f(x - h)) / (2h)\n",
    "\n",
    "  Rather than computing a full numeric gradient, we sparsely sample a few\n",
    "  dimensions along which to compute numeric derivatives.\n",
    "\n",
    "  Inputs:\n",
    "  - f: A function that inputs a torch tensor and returns a torch scalar\n",
    "  - x: A torch tensor giving the point at which to evaluate the numeric gradient\n",
    "  - analytic_grad: A torch tensor giving the analytic gradient of f at x\n",
    "  - num_checks: The number of dimensions along which to check\n",
    "  - h: Step size for computing numeric derivatives\n",
    "  \"\"\"\n",
    "  # fix random seed for \n",
    "  coutils.utils.fix_random_seed()\n",
    "\n",
    "  for i in range(num_checks):\n",
    "    \n",
    "    ix = tuple([random.randrange(m) for m in x.shape])\n",
    "    \n",
    "    oldval = x[ix].item()\n",
    "    x[ix] = oldval + h # increment by h\n",
    "    fxph = f(x).item() # evaluate f(x + h)\n",
    "    x[ix] = oldval - h # increment by h\n",
    "    fxmh = f(x).item() # evaluate f(x - h)\n",
    "    x[ix] = oldval     # reset\n",
    "\n",
    "    grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "    grad_analytic = analytic_grad[ix]\n",
    "    rel_error_top = abs(grad_numerical - grad_analytic)\n",
    "    rel_error_bot = (abs(grad_numerical) + abs(grad_analytic) + 1e-12)\n",
    "    rel_error = rel_error_top / rel_error_bot\n",
    "    msg = 'numerical: %f analytic: %f, relative error: %e'\n",
    "    print(msg % (grad_numerical, grad_analytic, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeRAY8TZAK22"
   },
   "outputs": [],
   "source": [
    "def train_linear_classifier(loss_func, W, X, y, learning_rate=1e-3, \n",
    "                            reg=1e-5, num_iters=100, batch_size=200, verbose=False):\n",
    "  \"\"\"\n",
    "  Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "  Inputs:\n",
    "  - loss_func: loss function to use when training. It should take W, X, y\n",
    "    and reg as input, and output a tuple of (loss, dW)\n",
    "  - W: A PyTorch tensor of shape (D, C) giving the initial weights of the\n",
    "    classifier. If W is None then it will be initialized here.\n",
    "  - X: A PyTorch tensor of shape (N, D) containing training data; there are N\n",
    "    training samples each of dimension D.\n",
    "  - y: A PyTorch tensor of shape (N,) containing training labels; y[i] = c\n",
    "    means that X[i] has label 0 <= c < C for C classes.\n",
    "  - learning_rate: (float) learning rate for optimization.\n",
    "  - reg: (float) regularization strength.\n",
    "  - num_iters: (integer) number of steps to take when optimizing\n",
    "  - batch_size: (integer) number of training examples to use at each step.\n",
    "  - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "  Returns: A tuple of:\n",
    "  - W: The final value of the weight matrix and the end of optimization\n",
    "  - loss_history: A list of Python scalars giving the values of the loss at each\n",
    "    training iteration.\n",
    "  \"\"\"\n",
    "  # assume y takes values 0...K-1 where K is number of classes\n",
    "  num_classes = torch.max(y) + 1\n",
    "  num_train, dim = X.shape\n",
    "  if W is None:\n",
    "    # lazily initialize W\n",
    "    W = 0.000001 * torch.randn(dim, num_classes, device=X.device, dtype=X.dtype)\n",
    "\n",
    "  # Run stochastic gradient descent to optimize W\n",
    "  loss_history = []\n",
    "  for it in range(num_iters):\n",
    "    X_batch = None\n",
    "    y_batch = None\n",
    "    \n",
    "    indices = torch.randint(low = 0, high = X.shape[0]-1, size=(batch_size,))\n",
    "    X_batch = X[indices]\n",
    "    y_batch = y[indices]\n",
    "    \n",
    "    # evaluate loss and gradient\n",
    "    loss, grad = loss_func(W, X_batch, y_batch, reg)\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # perform parameter update\n",
    "    W -= learning_rate * grad\n",
    "    \n",
    "    if verbose and it % 100 == 0:\n",
    "      print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "  return W, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdvAk57DD0xT"
   },
   "outputs": [],
   "source": [
    "def predict_linear_classifier(W, X):\n",
    "  \"\"\"\n",
    "  Use the trained weights of this linear classifier to predict labels for\n",
    "  data points.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A PyTorch tensor of shape (D, C), containing weights of a model\n",
    "  - X: A PyTorch tensor of shape (N, D) containing training data; there are N\n",
    "    training samples each of dimension D.\n",
    "\n",
    "  Returns:\n",
    "  - y_pred: PyTorch int64 tensor of shape (N,) giving predicted labels for each\n",
    "    elemment of X. Each element of y_pred should be between 0 and C - 1.\n",
    "  \"\"\"\n",
    "  y_pred = torch.zeros(X.shape[0])\n",
    "  score_pred = torch.mm(X, W)\n",
    "  y_pred = torch.argmax(score_pred, dim=1)\n",
    "  \n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1SfNxVs8OBv"
   },
   "outputs": [],
   "source": [
    "# Note: We will re-use `LinearClassifier' in Softmax section\n",
    "class LinearClassifier(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.W = None\n",
    "    \n",
    "  def train(self, X_train, y_train, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    train_args = (self.loss, self.W, X_train, y_train, learning_rate, reg,\n",
    "                  num_iters, batch_size, verbose)\n",
    "    self.W, loss_history = train_linear_classifier(*train_args)\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    return predict_linear_classifier(self.W, X) \n",
    "  \n",
    "  def loss(self, W, X_batch, y_batch, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative. \n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A PyTorch tensor of shape (D, C) containing (trained) weight of a model.\n",
    "    - X_batch: A PyTorch tensor of shape (N, D) containing a minibatch of N\n",
    "      data points; each point has dimension D.\n",
    "    - y_batch: A PyTorch tensor of shape (N,) containing labels for the minibatch.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an tensor of the same shape as W\n",
    "    \"\"\"\n",
    "    pass\n",
    "  def _loss(self, X_batch, y_batch, reg):\n",
    "    self.loss(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkuwyMY27RxS"
   },
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **visualize** the final learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLJMVGtvIgo3"
   },
   "source": [
    "First, let's start from implementing the naive softmax loss function with nested loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3MDCs7Cwss8"
   },
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, naive implementation (with loops).\n",
    "\n",
    "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "  of N examples.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A PyTorch tensor of shape (D, C) containing weights.\n",
    "  - X: A PyTorch tensor of shape (N, D) containing a minibatch of data.\n",
    "  - y: A PyTorch tensor of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "  - reg: (float) regularization strength\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss as single float\n",
    "  - gradient with respect to weights W; an tensor of same shape as W\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = torch.zeros_like(W)\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "  # Store the loss in loss and the gradient in dW. Don't forget the           #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "  \n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "\n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cER8fiSq7Ys-"
   },
   "source": [
    "As a sanity check to see whether we have implemented the loss correctly, run the softmax classifier with a small random weight matrix and no regularization. You should see loss near log(10) = 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9q77O7F7VI6"
   },
   "outputs": [],
   "source": [
    "# Generate a random softmax weight tensor and use it to compute the loss.\n",
    "coutils.utils.fix_random_seed()\n",
    "W = 0.0001 * torch.randn(3073, 10, device=data_dict['X_val'].device).double()\n",
    "\n",
    "X_batch = data_dict['X_val'][:128].double()\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to log(10.0).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (math.log(10.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QJzUHl5I0HH"
   },
   "source": [
    "Next, we use gradient checking to debug the analytic gradient of our naive softmax loss function. If you've implemented the gradient correctly, you should see relative errors less than `1e-6`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj6YpN3q1hVG"
   },
   "outputs": [],
   "source": [
    "coutils.utils.fix_random_seed()\n",
    "W = 0.0001 * torch.randn(3073, 10, device=data_dict['X_val'].device).double()\n",
    "X_batch = data_dict['X_val'][:128].double()\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "loss, grad = softmax_loss_naive(W, X_batch, y_batch, reg=0.0)\n",
    "\n",
    "f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg=0.0)[0]\n",
    "grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFcgeajBI-L3"
   },
   "source": [
    "Let's perform another gradient check with regularization enabled. Again you should see relative errors less than `1e-6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ik0i21sszZzg"
   },
   "outputs": [],
   "source": [
    "coutils.utils.fix_random_seed()\n",
    "W = 0.0001 * torch.randn(3073, 10, device=data_dict['X_val'].device).double()\n",
    "reg = 10.0\n",
    "\n",
    "X_batch = data_dict['X_val'][:128].double()\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "loss, grad = softmax_loss_naive(W, X_batch, y_batch, reg)\n",
    "\n",
    "f = lambda w: softmax_loss_naive(w, X_batch, y_batch, reg)[0]\n",
    "grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQgRzrdRJAm7"
   },
   "source": [
    "Then, let's move on to the vectorized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYlTPinzwv3x"
   },
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, vectorized version.  When you implment the \n",
    "  regularization over W, please DO NOT multiply the regularization term by 1/2 \n",
    "  (no coefficient). \n",
    "\n",
    "  Inputs and outputs are the same as softmax_loss_naive.\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = torch.zeros_like(W)\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "  # Store the loss in loss and the gradient in dW. Don't forget the           #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "    \n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "\n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88xZ0rbLJGKV"
   },
   "source": [
    "Now that we have a naive implementation of the softmax loss function and its gradient, implement a vectorized version in softmax_loss_vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGNAe-oP1dds"
   },
   "outputs": [],
   "source": [
    "coutils.utils.fix_random_seed()\n",
    "W = 0.0001 * torch.randn(3073, 10, device=data_dict['X_val'].device)\n",
    "reg = 0.05\n",
    "\n",
    "X_batch = data_dict['X_val'][:128]\n",
    "y_batch = data_dict['y_val'][:128]\n",
    "\n",
    "# Run and time the naive version\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_batch, y_batch, reg)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_naive = 1000.0 * (toc - tic)\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, ms_naive))\n",
    "\n",
    "# Run and time the vectorized version\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_batch, y_batch, reg)\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "ms_vec = 1000.0 * (toc - tic)\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vec, ms_vec))\n",
    "\n",
    "# we use the Frobenius norm to compare the two versions of the gradient.\n",
    "loss_diff = (loss_naive - loss_vec).abs().item()\n",
    "grad_diff = torch.norm(grad_naive - grad_vec, p='fro')\n",
    "print('Loss difference: %.2e' % loss_diff)\n",
    "print('Gradient difference: %.2e' % grad_diff)\n",
    "print('Speedup: %.2fX' % (ms_naive / ms_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqZScXKyq6WB"
   },
   "source": [
    "Let's check that your implementation of the softmax loss is numerically stable.\n",
    "\n",
    "If either of the following print `nan` then you should double-check the numeric stability of your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCyFPWxxq58R"
   },
   "outputs": [],
   "source": [
    "device = data_dict['X_train'].device\n",
    "dtype = torch.float32\n",
    "D = data_dict['X_train'].shape[1]\n",
    "C = 10\n",
    "\n",
    "W_ones = torch.ones(D, C, device=device, dtype=dtype)\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_naive, W_ones, \n",
    "                                       data_dict['X_train'], \n",
    "                                       data_dict['y_train'], \n",
    "                                       learning_rate=1e-8, reg=2.5e4,\n",
    "                                       num_iters=1, verbose=True)\n",
    "\n",
    "\n",
    "W_ones = torch.ones(D, C, device=device, dtype=dtype)\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_vectorized, W_ones, \n",
    "                                       data_dict['X_train'], \n",
    "                                       data_dict['y_train'], \n",
    "                                       learning_rate=1e-8, reg=2.5e4,\n",
    "                                       num_iters=1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR4JGKoek8FB"
   },
   "source": [
    "Now lets train a softmax classifier with some default hyperparameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kqga1rvjk7b8"
   },
   "outputs": [],
   "source": [
    "# fix random seed before we perform this operation\n",
    "coutils.utils.fix_random_seed(10)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "tic = time.time()\n",
    "\n",
    "W, loss_hist = train_linear_classifier(softmax_loss_vectorized, None, \n",
    "                                       data_dict['X_train'], \n",
    "                                       data_dict['y_train'], \n",
    "                                       learning_rate=1e-10, reg=2.5e4,\n",
    "                                       num_iters=1500, verbose=True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKjxCGwkorCc"
   },
   "source": [
    "Plot the loss curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K29x-DWNoujL"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_hist, 'o')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WvpBuJWSwfd"
   },
   "source": [
    "Let's compute the accuracy of current model. It should be less than 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zb8kY2MjSvfH"
   },
   "outputs": [],
   "source": [
    "# evaluate the performance on both the training and validation set\n",
    "y_train_pred = predict_linear_classifier(W, data_dict['X_train'])\n",
    "train_acc = 100.0 * (data_dict['y_train'] == y_train_pred).float().mean().item()\n",
    "print('training accuracy: %.2f%%' % train_acc)\n",
    "y_val_pred = predict_linear_classifier(W, data_dict['X_val'])\n",
    "val_acc = 100.0 * (data_dict['y_val'] == y_val_pred).float().mean().item()\n",
    "print('validation accuracy: %.2f%%' % val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuV0BZvzJirI"
   },
   "source": [
    "Now use the validation set to tune hyperparameters (regularization strength and learning rate). You should experiment with different ranges for the learning rates and regularization strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGyf3TkWB-Er"
   },
   "outputs": [],
   "source": [
    "class Softmax(LinearClassifier):\n",
    "  \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "  def loss(self, W, X_batch, y_batch, reg):\n",
    "    return softmax_loss_vectorized(W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68lmNVj31ddu"
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "learning_rates = [] # learning rate candidates\n",
    "regularization_strengths = [] # regularization strengths candidates\n",
    "\n",
    "# As before, store your cross-validation results in this dictionary.\n",
    "# The keys should be tuples of (learning_rate, regularization_strength) and\n",
    "# the values should be tuples (train_accuracy, val_accuracy)\n",
    "results = {}\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "#                                                                              #\n",
    "################################################################################\n",
    "    \n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efougAmNCFLo"
   },
   "source": [
    "Run the following to visualize your cross-validation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVhRe3-DBjPr"
   },
   "outputs": [],
   "source": [
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "plt.show()\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.gcf().set_size_inches(8, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Joo4RbeoKECC"
   },
   "source": [
    "Finally, let's visualize the learned weights for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDfxI7mR1ddz"
   },
   "outputs": [],
   "source": [
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(3, 32, 32, 10)\n",
    "w = w.transpose(0, 2).transpose(1, 0)\n",
    "\n",
    "w_min, w_max = torch.min(w), torch.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "\n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.type(torch.uint8).cpu())\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "28bBK1v7nJOu",
    "DkuwyMY27RxS"
   ],
   "name": "hw3WORKING.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
